{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In class exercise...\n",
    "* MI is biased in that small sample sizes lead to inaccurate estimates of PDFs, and that can sometimes lead to negative MI values (which should never happen in theory). \n",
    "* A common, and simple, approach, is to compute MI with shuffled condition labels (like randomization tests that we did many weeks back) and then subtract the shuffled MI from the actual MI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "# also define the default font we'll use for figures. \n",
    "fig_font = {'fontname':'Arial', 'size':'20'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    \"\"\"compute entropy of discrete array x\n",
    "\n",
    "    Args:\n",
    "        x (int): array of discrete values\n",
    "\n",
    "    Returns:\n",
    "        Hx (float): entropy of x\n",
    "\n",
    "    \"\"\"\n",
    "    # figure out unique values of x - can be more than just 0s, 1s\n",
    "    uniquex = np.unique(x)\n",
    "\n",
    "    Hx = 0\n",
    "    for i in np.arange(len(uniquex)):\n",
    "        # probability that x==uniquex[i]\n",
    "        px = np.sum(x==uniquex[i])/len(x)    \n",
    "\n",
    "        # check for px==0 because log2(0) = -inf\n",
    "        if px!=0:\n",
    "            Hx += (-np.sum( px * np.log2(px) ))  \n",
    "        else:\n",
    "            print('px is zero for value ', i)\n",
    "        \n",
    "    return Hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condEntropy(x,y):\n",
    "    \n",
    "    \"\"\"\n",
    "    conditional entropy, or the average entropy of x given each y, or Hxy\n",
    "    1) For all Y {i=1:numel(X)}, compute the entropy of X given each Y\n",
    "    2) Multiply H(X|Y==i) with the probability of each Y (i.e. pxi)\n",
    "    3) Sum over all i\n",
    "\n",
    "    Args:\n",
    "        x (int): array of discrete values\n",
    "        y (int): array of discrete values\n",
    "        \n",
    "    Returns:\n",
    "        Hxy (float): average conditional entropy of x given y\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Hxy=0\n",
    "    uniquex = np.unique(x)\n",
    "    uniquey = np.unique(y)\n",
    "\n",
    "    # loop over unique elements of y\n",
    "    for i in np.arange(len(uniquey)): \n",
    "\n",
    "        # probability that y==y(i) (prob of each y)\n",
    "        py = np.sum(y==uniquey[i]) / N\n",
    "\n",
    "        # then loop over all possible x's to compute entropy of x at each y\n",
    "        tmp=0\n",
    "        for j in np.arange(len(uniquex)):\n",
    "            px_y = np.sum((x==uniquex[j]) & (y==uniquey[i])) / np.sum(y==uniquey[i])    # e.g. prob x==1 when y==0\n",
    "            tmp += (-( px_y * np.log2(px_y) ))                                     # entropy      \n",
    "\n",
    "        # then tally up entropy of x given each specific y multiplied by the probability of that y (py)\n",
    "        Hxy += py*tmp\n",
    "\n",
    "    return Hxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First set up two arrays of data...make them correlated to some degree so that there is a reasonably high MI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal values: 744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.48734155],\n",
       "       [0.48734155, 1.        ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two discrete, uncorrelated arrays filled with 0's and 1's\n",
    "\n",
    "N = 1000   # number of data points\n",
    "x = np.round(np.random.rand(N))\n",
    "y = np.round(np.random.rand(N))\n",
    "\n",
    "corrN = 5000\n",
    "for i in np.arange(corrN):\n",
    "    rand = np.random.randint(1,501)\n",
    "    y[rand] = x[rand]\n",
    "\n",
    "temp = 0\n",
    "for j in np.arange(N):\n",
    "    if x[j] == y[j]:\n",
    "        temp += 1\n",
    "print(\"Equal values:\", temp)\n",
    "\n",
    "np.corrcoef(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then compute the MI between the arrays. Can do two discrete arrays for simplicity, and import the entropy and conditional entropy functions from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MI is:  0.17437316254305535\n"
     ]
    }
   ],
   "source": [
    "Hx = entropy(x=x)\n",
    "Hxy = condEntropy(x=x,y=y)\n",
    "print('MI is: ', Hx-Hxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now repeat the above operations, but shuffle the data arrays and repeat the analysis many times (~500-1000 times). Plot the distribution of MI values that you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of bootstraps\n",
    "num_bootstraps = 1000\n",
    "MI = np.zeros(num_bootstraps)\n",
    "\n",
    "for i in np.arange(num_bootstraps):\n",
    "    # with replacement generate a sample number from 0:N exclusive and do that N times\n",
    "    index = np.random.randint(N, size=N)\n",
    "\n",
    "    # use that to pull data from each of our arrays\n",
    "    tmp1 = x[index]\n",
    "    tmp2 = y[index]\n",
    "    \n",
    "    # compute correlation\n",
    "    Hx = entropy(x=tmp1)\n",
    "    Hxy = condEntropy(x=tmp1,y=tmp2)\n",
    "    MI[i] = Hx-Hxy\n",
    "    \n",
    "# then compute 95% CIs based on percentiles \n",
    "CIs = np.percentile(corr, [2.5, 97.5])\n",
    "\n",
    "# histogram it\n",
    "plt.hist(MI, color='r', alpha=1, bins=30)\n",
    "plt.xlabel('MI')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now subtract the mean of the shuffled MI values from your 'real' MI value...this will help correct for any bias that is introduced by a limited sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MI: 0.17571846986929177\n",
      "Difference: -0.01113747053829775\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean MI:\", np.mean(MI))\n",
    "\n",
    "print(\"Difference:\", Hx-Hxy - np.mean(MI))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
